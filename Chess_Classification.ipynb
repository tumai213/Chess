{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d2dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter-git\n",
      "  Downloading jupyter-git-1.0.22.tar.gz (6.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting peppercorn (from jupyter-git)\n",
      "  Downloading peppercorn-0.6-py3-none-any.whl (4.8 kB)\n",
      "Building wheels for collected packages: jupyter-git\n",
      "  Building wheel for jupyter-git (setup.py): started\n",
      "  Building wheel for jupyter-git (setup.py): finished with status 'done'\n",
      "  Created wheel for jupyter-git: filename=jupyter_git-1.0.22-py3-none-any.whl size=6313 sha256=392418f57d616aef66c1b447f4cc70f7fdb0ffeee819a20527764489881f6793\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\93\\96\\cc\\a9d5f93ebcf59d7ff92d37fa892f5e7a38b521ceac13f72a9f\n",
      "Successfully built jupyter-git\n",
      "Installing collected packages: peppercorn, jupyter-git\n",
      "Successfully installed jupyter-git-1.0.22 peppercorn-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyter-git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f1df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abc\n",
    "#asdsahdbs\n",
    "# assdsad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f870f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading numpy-1.26.4-cp39-cp39-win_amd64.whl.metadata (61 kB)\n",
      "     -------------------------------------- 61.0/61.0 kB 819.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (21.3)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading protobuf-4.25.3-cp39-cp39-win_amd64.whl.metadata (541 bytes)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "google-api-core 1.25.1 requires google-auth<2.0dev,>=1.21.1, but you have google-auth 2.28.0 which is incompatible.\n",
      "google-cloud-core 1.7.1 requires google-auth<2.0dev,>=1.24.0, but you have google-auth 2.28.0 which is incompatible.\n",
      "google-cloud-storage 1.31.0 requires google-auth<2.0dev,>=1.11.0, but you have google-auth 2.28.0 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.26.4 which is incompatible.\n",
      "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.26.4 which is incompatible.\n",
      "streamlit 1.28.0 requires typing-extensions<5,>=4.3.0, but you have typing-extensions 4.1.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.42.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.37.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading grpcio-1.60.1-cp39-cp39-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.4)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading protobuf-4.23.4-cp39-cp39-win_amd64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.15.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.7.2)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading google_auth-2.28.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Downloading numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 15.8/15.8 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.60.1-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "   ---------------------------------------- 3.7/3.7 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.23.4-cp39-cp39-win_amd64.whl (422 kB)\n",
      "   ---------------------------------------- 422.5/422.5 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading google_auth-2.28.0-py2.py3-none-any.whl (186 kB)\n",
      "   ---------------------------------------- 186.9/186.9 kB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf, numpy, grpcio, google-auth\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.1\n",
      "    Uninstalling protobuf-3.19.1:\n",
      "      Successfully uninstalled protobuf-3.19.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.42.0\n",
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.33.0\n",
      "    Uninstalling google-auth-1.33.0:\n",
      "      Successfully uninstalled google-auth-1.33.0\n",
      "Successfully installed google-auth-2.26.2 grpcio-1.60.0 numpy-1.26.3 protobuf-4.23.4\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55001fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6dd6dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow\n",
      "  Downloading pillow-10.2.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 522.2 kB/s eta 0:00:00\n",
      "Installing collected packages: pillow\n",
      "Successfully installed pillow-10.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d4c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pillow là thư viện thao tác với hình ảnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b1c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b4d70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os là thư viện tương tác với hệ điều hình "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1d2d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94c529a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList = os.listdir(\"C:/Users/ADMIN/Downloads/chess_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74e77d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bishop_resized', 'knight-resize', 'pawn_resized', 'Queen-Resized', 'Rook-resize']\n"
     ]
    }
   ],
   "source": [
    "print(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f45cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction= \"C:/Users/ADMIN/Downloads/chess_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81a62147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new train data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26900a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData= os.path.join(direction,\"train\")\n",
    "os.mkdir(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a20a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBishopData= os.path.join(trainData,\"Bishop\")\n",
    "os.mkdir(trainBishopData)\n",
    "\n",
    "trainPawnData= os.path.join(trainData,\"Pawn\")\n",
    "os.mkdir(trainPawnData)\n",
    "\n",
    "trainKnightData= os.path.join(trainData,\"Knight\")\n",
    "os.mkdir(trainKnightData)\n",
    "\n",
    "trainRookData= os.path.join(trainData,\"Rook\")\n",
    "os.mkdir(trainRookData)\n",
    "\n",
    "trainQueenData= os.path.join(trainData,\"Queen\")\n",
    "os.mkdir(trainQueenData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b45d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new validation data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77b7e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "validData= os.path.join(direction,\"validation\")\n",
    "os.mkdir(validData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdc30e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "validBishopData= os.path.join(validData,\"Bishop\")\n",
    "os.mkdir(validBishopData)\n",
    "\n",
    "validBishopData= os.path.join(validData,\"Pawn\")\n",
    "os.mkdir(validBishopData)\n",
    "\n",
    "validBishopData= os.path.join(validData,\"Knight\")\n",
    "os.mkdir(validBishopData)\n",
    "\n",
    "validBishopData= os.path.join(validData,\"Rook\")\n",
    "os.mkdir(validBishopData)\n",
    "\n",
    "validBishopData= os.path.join(validData,\"Queen\")\n",
    "os.mkdir(validBishopData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd0e6be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Thư viện thao tác với các files và đường dẫn\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4a79dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size =0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e466b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bishop direction\n",
    "Bishop_source= \"C:/Users/ADMIN/Downloads/chess_data/bishop_resized/\"\n",
    "Bishop_train= \"C:/Users/ADMIN/Downloads/chess_data/train/Bishop/\"\n",
    "Bishop_valid= \"C:/Users/ADMIN/Downloads/chess_data/validation/Bishop/\"\n",
    "\n",
    "# Knight direction\n",
    "Knight_source= \"C:/Users/ADMIN/Downloads/chess_data/knight-resize/\"\n",
    "Knight_train= \"C:/Users/ADMIN/Downloads/chess_data/train/Knight/\"\n",
    "Knight_valid= \"C:/Users/ADMIN/Downloads/chess_data/validation/Knight/\"\n",
    "\n",
    "# Pawn direction\n",
    "Pawn_source= \"C:/Users/ADMIN/Downloads/chess_data/pawn_resized/\"\n",
    "Pawn_train= \"C:/Users/ADMIN/Downloads/chess_data/train/Pawn/\"\n",
    "Pawn_valid= \"C:/Users/ADMIN/Downloads/chess_data/validation/Pawn/\"\n",
    "\n",
    "# Rook direction\n",
    "Rook_source= \"C:/Users/ADMIN/Downloads/chess_data/Rook-resize/\"\n",
    "Rook_train= \"C:/Users/ADMIN/Downloads/chess_data/train/Rook/\"\n",
    "Rook_valid= \"C:/Users/ADMIN/Downloads/chess_data/validation/Rook/\"\n",
    "\n",
    "# Queen direction\n",
    "Queen_source= \"C:/Users/ADMIN/Downloads/chess_data/Queen-Resized/\"\n",
    "Queen_train= \"C:/Users/ADMIN/Downloads/chess_data/train/Queen/\"\n",
    "Queen_valid= \"C:/Users/ADMIN/Downloads/chess_data/validation/Queen/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f90ec25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(source, training, validation, split_size):\n",
    "    files =[]\n",
    "    for filename in os.listdir(source):\n",
    "        file = source + filename\n",
    "        if os.path.getsize(file) >0:\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            print(filename + \"cc\")\n",
    "    training_length = int(len(files) *split_size)\n",
    "    valid_length = int(len(files) - training_length)\n",
    "    # random files\n",
    "    shuffleSet = random.sample(files, len(files))\n",
    "    \n",
    "    trainSet= shuffleSet[0:training_length]\n",
    "    validSet= shuffleSet[training_length:]\n",
    "    # copy the train image\n",
    "    for filename in trainSet:\n",
    "        this = source + filename\n",
    "        destination = training + filename\n",
    "        shutil.copyfile(this, destination)\n",
    "    # copy the valid image\n",
    "    for filename in validSet:\n",
    "        this = source + filename\n",
    "        destination = validation + filename\n",
    "        shutil.copyfile(this, destination)\n",
    "split_data(Bishop_source, Bishop_train, Bishop_valid,split_size)\n",
    "split_data(Knight_source, Knight_train, Knight_valid,split_size)\n",
    "split_data(Pawn_source, Pawn_train, Pawn_valid,split_size)\n",
    "split_data(Rook_source, Rook_train, Rook_valid,split_size)\n",
    "split_data(Queen_source, Queen_train, Queen_valid,split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1e23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= \"C:/Users/ADMIN/Downloads/chess_data/King\"\n",
    "output_folder = \"C:/Users/ADMIN/Downloads/chess_data/king-resize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e406ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(folder_path, max_width, max_height, output_folder=None, quality=90):\n",
    "  \"\"\"\n",
    "  Resizes all images in a folder using Pillow.\n",
    "\n",
    "  Args:\n",
    "    folder_path (str): Path to the folder containing images.\n",
    "    max_width (int): Maximum width for resized images.\n",
    "    max_height (int): Maximum height for resized images.\n",
    "    output_folder (str, optional): Path to save resized images. Defaults to the same folder.\n",
    "    aspect_ratio (bool, optional): Maintain aspect ratio while resizing. Defaults to True.\n",
    "    quality (int, optional): Compression quality for JPEG images (1-100). Defaults to 90.\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "\n",
    "  # Check if folder exists\n",
    "  if not os.path.isdir(folder_path):\n",
    "    raise ValueError(f\"Folder not found: {folder_path}\")\n",
    "\n",
    "  # Create output folder if needed\n",
    "  if output_folder is None:\n",
    "    output_folder = folder_path\n",
    "\n",
    "  if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "  # Process each image in the folder\n",
    "  for filename in os.listdir(folder_path):\n",
    "    # Get full path and check if it's a file\n",
    "    filepath = os.path.join(folder_path, filename)\n",
    "    if not os.path.isfile(filepath):\n",
    "      continue\n",
    "\n",
    "    # Open the image\n",
    "    try:\n",
    "      img = Image.open(filepath)\n",
    "    except Exception as e:\n",
    "      print(f\"Error opening image: {filepath}. Skipping...\")\n",
    "      continue\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize((max_width, max_height))\n",
    "    resized_img = resized_img.convert(\"RGB\")\n",
    "    # Save the resized image\n",
    "    output_filename = os.path.join(output_folder, os.path.splitext(filename)[0] + \"_resized.jpg\")\n",
    "    resized_img.save(output_filename, \"JPEG\", quality=quality)\n",
    "\n",
    "  print(f\"Resized images saved to: {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe8197c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized images saved to: C:/Users/ADMIN/Downloads/chess_data/king-resize\n"
     ]
    }
   ],
   "source": [
    "resize_images(path, 224, 224, output_folder=output_folder, quality=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8483e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c9d836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width= 224\n",
    "img_height= 224\n",
    "batch_size = 32\n",
    "epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1fbdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = \"C:/Users/ADMIN/Downloads/chess_data/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62369b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "NumOfClasses = len(glob('C:/Users/ADMIN/Downloads/chess_data/train/*'))\n",
    "print(NumOfClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40f1733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 550 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# over sampling\n",
    " #rescale để chuyển dữ liệu về khoảng 0-1 \n",
    "train_datagen= ImageDataGenerator(rescale=1/255.0, rotation_range=30, zoom_range=0.4, horizontal_flip=True, shear_range=0.4)\n",
    "# load image\n",
    "train_generator = train_datagen.flow_from_directory(training_dir,batch_size=batch_size,class_mode='categorical', target_size=(img_height,img_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d82db84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_dir = \"C:/Users/ADMIN/Downloads/chess_data/validation\"\n",
    "valid_datagen = ImageDataGenerator(rescale=1/255.0)\n",
    "valid_generator = valid_datagen.flow_from_directory(valid_dir,batch_size=batch_size,class_mode='categorical', target_size=(img_height,img_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e73d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback\n",
    "callback = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "bestModelFileName = \"C:/Users/ADMIN/Downloads/chess_data/chest_best_model.h5\"\n",
    "bestModel = ModelCheckpoint(bestModelFileName, monitor='val_accuracy', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74012dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 111, 111, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 52, 52, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 26, 26, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 12, 12, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 10, 10, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 5, 5, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6400)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               3277312   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3967877 (15.14 MB)\n",
      "Trainable params: 3967877 (15.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "model = Sequential(\n",
    "    [Conv2D(32,(3,3), activation='relu', input_shape=(img_height, img_width,3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(64,(3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(64,(3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(128,(3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(256,(3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    \n",
    "    Dense(NumOfClasses, activation='softmax')]\n",
    ")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0beee1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.6031 - accuracy: 0.2164\n",
      "Epoch 1: val_accuracy improved from -inf to 0.23762, saving model to C:/Users/ADMIN/Downloads/chess_data\\chest_best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 18s 879ms/step - loss: 1.6031 - accuracy: 0.2164 - val_loss: 1.5911 - val_accuracy: 0.2376\n",
      "Epoch 2/100\n",
      "15/18 [========================>.....] - ETA: 2s - loss: 1.5872 - accuracy: 0.2709"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with Adam Optimizer\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_generator, epochs=epochs, verbose=1, validation_data=valid_generator, callbacks=bestModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca7106da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Result\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "# visualize accuracy chart\n",
    "fig = plt.figure(figsize=(14,7))\n",
    "plt.plot(epochs, acc, 'r', label='Train Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy chart')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# visualize loss chart\n",
    "fig1 = plt.figure(figsize=(14,7))\n",
    "plt.plot(epochs, loss, 'r', label='Train loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss chart')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7372430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "classes = ['Queen', 'Rook', 'Pawn', 'Bishop', 'Knight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c34e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model(\"C:/Users/ADMIN/Downloads/chess_data/chest_best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "849e1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image preprocessing\n",
    "def image_preprocessing(path):\n",
    "    image = load_img(path, target_size=(image_height, image_width))\n",
    "    image_result = img_to_array(image)\n",
    "    image_result = np.expand_dims(image_result, axis = 0)\n",
    "    image_result =image_result/255.0\n",
    "    return image_result\n",
    "test_path = \"C:/Users/ADMIN/Downloads/chess_data/Test/pawn3.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2082c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageforModel = image_preprocessing(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4daa5c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(imageforModel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bab58fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "resultArray = model.predict(imageforModel, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05f91b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "result = np.argmax(resultArray, axis=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b223d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=classes[result[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68395dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(test_path)\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "resized_img = cv2.resize(img, (300, int(img.shape[0] * 300 / img.shape[1])))\n",
    "cv2.putText(resized_img, text,(0,100),font, 2, (255,0,0), 4)\n",
    "\n",
    "cv2.imshow('chess',resized_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8867d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP\n",
    "model2 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ca5cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.add(Dense(256, input_shape=(img_height, img_width,3), activation=\"sigmoid\"))\n",
    "model2.add(Dense(128, activation=\"sigmoid\"))\n",
    "model2.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d9d9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c065110",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c7578f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network...\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5820 - accuracy: 0.2673\n",
      "Epoch 1: val_accuracy improved from 0.23762 to 0.26733, saving model to C:/Users/ADMIN/Downloads/chess_data\\chest_best_model.h5\n",
      "18/18 [==============================] - 13s 720ms/step - loss: 1.5820 - accuracy: 0.2673 - val_loss: 1.5787 - val_accuracy: 0.2673\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 1.5809 - accuracy: 0.2673\n",
      "Epoch 2: val_accuracy did not improve from 0.26733\n",
      "18/18 [==============================] - 13s 692ms/step - loss: 1.5809 - accuracy: 0.2673 - val_loss: 1.5786 - val_accuracy: 0.2673\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5807 - accuracy: 0.2673\n",
      "Epoch 3: val_accuracy did not improve from 0.26733\n",
      "18/18 [==============================] - 14s 777ms/step - loss: 1.5807 - accuracy: 0.2673 - val_loss: 1.5784 - val_accuracy: 0.2673\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.5800 - accuracy: 0.2673\n",
      "Epoch 4: val_accuracy did not improve from 0.26733\n",
      "18/18 [==============================] - 15s 813ms/step - loss: 1.5800 - accuracy: 0.2673 - val_loss: 1.5783 - val_accuracy: 0.2673\n",
      "Epoch 5/100\n",
      "13/18 [====================>.........] - ETA: 3s - loss: 1.5842 - accuracy: 0.2769"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining network...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbestModel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training network...\")\n",
    "H = model.fit(train_generator, validation_data=valid_generator, epochs=epochs,\n",
    "batch_size=batch_size, callbacks=bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa3839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
